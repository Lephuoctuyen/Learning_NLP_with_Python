{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8BARDNDk6awi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM5Nce-fBsxE",
        "outputId": "f63fcf97-04af-4bfd-cfe4-8fdeb76ceb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Python-Natural-Language-Processing-Cookbook-Second-Edition'...\n",
            "remote: Enumerating objects: 433, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 433 (delta 11), reused 6 (delta 2), pack-reused 409 (from 1)\u001b[K\n",
            "Receiving objects: 100% (433/433), 18.28 MiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as npx`\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertForSequenceClassification\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from Chapter04.svm_classification import split_dataset\n",
        "from Chapter05.twitter_sentiment import clean_data, plot_model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "DATASET_SIZE = 4000\n",
        "english_twitter = \"Chapter05/twitter_english.csv\"\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "max_length = 200\n",
        "\n",
        "\n",
        "\n",
        "def map_inputs_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label\n",
        "\n",
        "def encode_example(input_text):\n",
        "    tokenized = tokenizer.tokenize(input_text)\n",
        "    bert_input = tokenizer.encode_plus(\n",
        "                    input_text,\n",
        "                    add_special_tokens = True, # add [CLS], [SEP]\n",
        "                    max_length = max_length, # max length of the text that can go to BERT\n",
        "                    pad_to_max_length = True, # add [PAD] tokens\n",
        "                    return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                    return_tensors='tf'\n",
        "        )\n",
        "    return bert_input\n",
        "\n",
        "def encode_data(df):\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "    for index, row in df.iterrows():\n",
        "        tweet = row['tweet']\n",
        "        label = row['sentiment']\n",
        "        bert_input = tokenizer.encode_plus(\n",
        "                        tweet,\n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        max_length = max_length, # max length of the text that can go to BERT\n",
        "                        pad_to_max_length = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "            )\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append([label])\n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_inputs_to_dict)\n",
        "\n",
        "\n",
        "def prepare_dataset(df, size=int(DATASET_SIZE/2)):\n",
        "    df = clean_data(df)\n",
        "    df = pd.concat([df.head(size),df.tail(size)])\n",
        "    df = df.sample(frac = 1)\n",
        "    ds = encode_data(df)\n",
        "    return ds\n",
        "\n",
        "def load_existing_model(export_dir):\n",
        "    model = TFBertForSequenceClassification.from_pretrained(export_dir)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_test_train_val_datasets(ds, size=DATASET_SIZE, batch_size=BATCH_SIZE):\n",
        "    ds.shuffle(32)\n",
        "    train_size = int(0.7 * size)\n",
        "    val_size = int(0.15 * size)\n",
        "    test_size = int(0.15 * size)\n",
        "    train_dataset = ds.take(train_size).batch(batch_size)\n",
        "    test_dataset = ds.skip(train_size)\n",
        "    val_dataset = test_dataset.skip(test_size).batch(batch_size)\n",
        "    test_dataset = test_dataset.take(test_size).batch(batch_size)\n",
        "    return (train_dataset, test_dataset, val_dataset)\n",
        "\n",
        "def fine_tune_model(ds, export_dir):\n",
        "    (train_dataset, test_dataset, val_dataset) = get_test_train_val_datasets(ds)\n",
        "    learning_rate = 2e-5\n",
        "    number_of_epochs = 1\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "    bert_history = model.fit(train_dataset, epochs=number_of_epochs, validation_data=val_dataset)\n",
        "    model.save_pretrained(export_dir)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = []\n",
        "    for tweet in X_test:\n",
        "        bert_input = encode_example(tweet)\n",
        "        tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
        "        tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
        "        new_label = np.argmax(tf_pred, axis=-1)\n",
        "        y_pred.append(new_label)\n",
        "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))\n",
        "\n",
        "def test_new_example(model_path, tweet):\n",
        "    model = load_existing_model(model_path)\n",
        "    bert_input = encode_example(tweet)\n",
        "    tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
        "    tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
        "    new_label = np.argmax(tf_pred, axis=-1)\n",
        "    print(new_label)\n",
        "    return new_label\n",
        "\n",
        "def load_and_evaluate_existing_model(export_dir, num_points=200):\n",
        "    model = load_existing_model(export_dir)\n",
        "    df = pd.read_csv(english_twitter, encoding=\"latin1\")\n",
        "    df = clean_data(df)\n",
        "    df = pd.concat([df.head(num_points),df.tail(num_points)])\n",
        "    (X_train, X_test, y_train, y_test) = split_dataset(df, 'tweet', 'sentiment')\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(english_twitter, encoding=\"latin1\")\n",
        "    dataset = prepare_dataset(df)\n",
        "    model = fine_tune_model(dataset, 'Chapter05/bert_twitter_test2_model')\n",
        "\n",
        "\n",
        "if(__name__ == \"__main__\"):\n",
        "    main()\n",
        "    #test_new_example('Chapter04/bert_twitter_test_model', \"I hate going to school\")\n",
        "    #load_and_evaluate_existing_model('Chapter05/bert_twitter_test2_model')"
      ],
      "metadata": {
        "id": "PTInWFQIAidU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence, DatasetDict\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from evaluate import load\n",
        "music_ner_df = pd.read_csv('../data/music_ner.csv')\n",
        "def change_label(input_label):\n",
        "    input_label = input_label.replace(\"_deduced\", \"\")\n",
        "    return input_label\n",
        "music_ner_df[\"label\"] = music_ner_df[\"label\"].apply(change_label)\n",
        "music_ner_df[\"text\"] = music_ner_df[\"text\"].apply(lambda x: x.replace(\"|\", \",\"))\n",
        "print(music_ner_df)"
      ],
      "metadata": {
        "id": "K9c6HIEzBCfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = list(set(music_ner_df[\"id\"].values))\n",
        "docs = {}\n",
        "for id in ids:\n",
        "    entity_rows = music_ner_df.loc[music_ner_df['id'] == id]\n",
        "    text = entity_rows.head(1)[\"text\"].values[0]\n",
        "    doc = small_model(text)\n",
        "    ents = []\n",
        "    for index, row in entity_rows.iterrows():\n",
        "        label = row[\"label\"]\n",
        "        start = row[\"start_offset\"]\n",
        "        end = row[\"end_offset\"]\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        ents.append(span)\n",
        "    doc.ents = ents\n",
        "    docs[doc.text] = doc\n",
        "data_file = \"../data/music_ner_bio.bio\"\n",
        "tag_mapping = {\"O\": 0, \"B-Artist\": 1, \"I-Artist\": 2, \"B-WoA\": 3, \"I-WoA\": 4}\n",
        "with open(data_file) as f:\n",
        "    data = f.read()\n",
        "tokens = []\n",
        "ner_tags = []\n",
        "spans = []\n",
        "sentences = data.split(\"\\n\\n\")\n",
        "for sentence in sentences:\n",
        "    words = []\n",
        "    tags = []\n",
        "    this_sentence_spans = []\n",
        "    word_tag_pairs = sentence.split(\"\\n\")\n",
        "    for pair in word_tag_pairs:\n",
        "        (word, tag) = pair.split(\"\\t\")\n",
        "        words.append(word)\n",
        "        tags.append(tag_mapping[tag])\n",
        "    sentence_text = \" \".join(words)\n",
        "    try:\n",
        "        doc = docs[sentence_text]\n",
        "    except:\n",
        "        pass\n",
        "    ent_dict = {}\n",
        "    for ent in doc.ents:\n",
        "        this_sentence_spans.append(f\"{ent.label_}: {ent.text}\")\n",
        "    tokens.append(words)\n",
        "    ner_tags.append(tags)\n",
        "    spans.append(this_sentence_spans)\n",
        "indices = range(0, len(spans))\n",
        "train, test = train_test_split(indices, test_size=0.1)\n",
        "train_tokens = []\n",
        "test_tokens = []\n",
        "train_ner_tags = []\n",
        "test_ner_tags = []\n",
        "train_spans = []\n",
        "test_spans = []\n",
        "for i, (token, ner_tag, span) in enumerate(zip(tokens, ner_tags, spans)):\n",
        "    if i in train:\n",
        "        train_tokens.append(token)\n",
        "        train_ner_tags.append(ner_tag)\n",
        "        train_spans.append(span)\n",
        "    else:\n",
        "        test_tokens.append(token)\n",
        "        test_ner_tags.append(ner_tag)\n",
        "        test_spans.append(span)\n",
        "\n",
        "print(len(train_spans))\n",
        "print(len(test_spans))\n",
        "539\n",
        "60\n",
        "training_df = pd.DataFrame({\"tokens\":train_tokens, \"ner_tags\": train_ner_tags, \"spans\": train_spans})\n",
        "test_df = pd.DataFrame({\"tokens\": test_tokens, \"ner_tags\": test_ner_tags, \"spans\": test_spans})\n",
        "training_df[\"text\"] = training_df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "test_df[\"text\"] = test_df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "training_df.dropna()\n",
        "test_df.dropna()\n",
        "print(test_df)"
      ],
      "metadata": {
        "id": "U4krZ-0cBDBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "#model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "features = Features({'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
        "            'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-Artist', 'I-Artist', 'B-WoA', 'I-WoA'], id=None), length=-1, id=None),\n",
        "            'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
        "            'text': Value(dtype='string', id=None)\n",
        "                    })\n",
        "training_dataset = Dataset.from_pandas(training_df, features=features)\n",
        "test_dataset = Dataset.from_pandas(test_df, features=features)\n",
        "dataset = DatasetDict({\"train\":training_dataset, \"test\":test_dataset})\n",
        "print(dataset[\"train\"].features)\n",
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "_jFoIVOWBOZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_adjust_labels(all_samples_per_split):\n",
        "    tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"text\"])\n",
        "    total_adjusted_labels = []\n",
        "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
        "        prev_wid = -1\n",
        "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
        "        existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
        "        i = -1\n",
        "        adjusted_label_ids = []\n",
        "        for wid in word_ids_list:\n",
        "            if (wid is None):\n",
        "                adjusted_label_ids.append(-100)\n",
        "            elif (wid != prev_wid):\n",
        "                i = i + 1\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "                prev_wid = wid\n",
        "            else:\n",
        "                label_name = label_names[existing_label_ids[i]]\n",
        "                adjusted_label_ids.append(existing_label_ids[i])\n",
        "        total_adjusted_labels.append(adjusted_label_ids)\n",
        "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
        "    return tokenized_samples\n",
        "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)"
      ],
      "metadata": {
        "id": "WY2BOc0ZBOxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load(\"seqeval\")\n",
        "def compute_metrics(data):\n",
        "    predictions, labels = data\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    data = zip(predictions, labels)\n",
        "    data = [[(p, l) for (p, l) in zip(prediction, label) if l != -100] for prediction, label in data]\n",
        "\n",
        "    true_predictions = [[label_names[p] for (p, l) in data_point] for data_point in data]\n",
        "    true_labels = [[label_names[l] for (p, l) in data_point] for data_point in data]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    flat_results = {\n",
        "        \"overall_precision\": results[\"overall_precision\"],\n",
        "        \"overall_recall\": results[\"overall_recall\"],\n",
        "        \"overall_f1\": results[\"overall_f1\"],\n",
        "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "    for k in results.keys():\n",
        "      if (k not in flat_results.keys()):\n",
        "        flat_results[k + \"_f1\"] = results[k][\"f1\"]\n",
        "\n",
        "    return flat_results\n",
        "# Train model\n",
        "model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_names))\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tune_bert_output\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=7,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps = 1000,\n",
        "    run_name = \"ep_10_tokenized_11\",\n",
        "    save_strategy='no'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ARig7YhUBVh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "XKR9LjbcBW95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "trainer.save_model(\"../models/bert_fine_tuned\")\n",
        "# Use model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"../models/bert_fine_tuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"../models/bert_fine_tuned\")\n",
        "text = \"music similar to morphine robocobra quartet | featuring elements like saxophone prominent bass\"\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "pipe(text)\n",
        "# tag_mapping = {\"O\": 0, \"B-Artist\": 1, \"I-Artist\": 2, \"B-WoA\": 3, \"I-WoA\": 4}"
      ],
      "metadata": {
        "id": "EK7uDRkMBZ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hc-eiLADBb53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}